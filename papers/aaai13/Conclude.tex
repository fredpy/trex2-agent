\section{Discussion and Future work}
\label{sec:conclude}

Increased robustness in hardware has led to persistence of robots
often in hostile environments such as our oceans. The versatility of
onboard autonomy has resulted in the need to anticipate and adapt to
goals during mission execution.  Most existing autonomy approaches
fall short of dealing with the ensuing dichotomy of proactive dispatch
versus deferment.  Typically, the solution pursued is to integrate
such knowledge within the system domain description which often
results in a more complex model making it harder to maintain and
therefore error prone.  This motivates our approach to provide an
efficient solution to deduce execution policy based on the
supplementary information namely that of defined policies in the plan.
The work in this paper focuses on plan structure in abstraction
without direct consideration of timing constraints applied to each
goal in the plan. We are able to use breadth-first search within the
existing plan structure to select different execution policies for
each token within the plan. By doing so, we can improve overall
mission execution by avoiding proactivity that could lead to
inefficiency in handling new goal requests.

A number of open issues remain to be explored, however. The causal
link traversal proposed is agnostic to the temporal constraints
applied to timepoints of the plan. This design choice was made as we
consider our approach as a heuristic supporting executive
decisions. Therefore the management of the temporal constraints and
their propagation remained the reponsibility of the executive itself.
Still, including temporal constraints could greatly increase the
quality of our solution.  For instance, in Fig. \ref{fig:ex:mixed2},
where the consequent was a {\em Surface} token, planning instantiated
the goal of surveying \emph{Vent1}. Currently, the result would make
all tokens related to the {\em Surface} token proactive due to the
goal immediately following it.  This can potentially lead to premature
surfacing which would prohibit the agent from acting on goals
until 13:00. Typically the {\em Surface}, which is to end after 13:00,
is acting as a guard on the propagation.  Therefore, ideally only
those tokens that are directly after this {\em Surface} token should
have been marked as proactive as shown in Fig. \ref{fig:res:ideal},
while keeping actions preceding 13:00 deferred.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{figs/example_ideal}
  \vskip-3mm
  \caption{\small The ideal plan solution for
    Fig. \ref{fig:ex:mixed2}. The thick line at 11:30 indicates
    current time. Since {\em At Surface} is constrained to be at
    13:00, the AUV would be stuck for $20$ minutes.}
  \label{fig:res:ideal}
  \vskip-3mm
\end{figure}
 
Such refinement can be done only by considering the temporal
% constraints applied to plan timepoints. Should the {\em Surface} not
% have been constrained to be around 13:00 pm starting its execution
% proactively would have not blocked the vehicle at the surface. We
% plan, in the future, to explore how both the marking of the goals {\em
%   as urgent} and propagation to action can be better deduced by
% analyzing the simple temporal network (STN) supporting the current
% plan. Further, as the planner we used did not implement STNU and the
% work presented in \cite{morris01} about dynamic controllability. We
% believe that as this work is handled during planning while our
% approach is handled during plan execution, both should be easily
% complementary still the {\bf wait} actions inserted by their algorithm
% may need to be considered with a non urgent status within our
% approach.
constraints applied to plan timepoints. Should the {\em Surface} not
have been constrained to be around 13:00, starting its execution
proactively would not have blocked the vehicle at the surface. We hope
to explore how the analysis of the simple temporal network supporting
the plan can help identify and deduce a token's execution policy.  The
insertion of {\bf wait} actions during planning in \cite{morris01} and
the possibility of using STNUs (STNs with uncertainty) within our plan
representation, also needs investigation with extensions to execution
time changes in plan structure.

% Further, the planner we used did
% not implement STNU or the work presented in \cite{morris01} about
% dynamic controllability. We believe that as this work is handled
% during planning while our approach is handled during plan execution
% both should be easily complementary. Still the {\bf wait} actions
% inserted by their algorithm may need to be considered with a non
% urgent status within our approach.

These refinements are likely to improve overall execution in our agent
despite uncertainty in the number and nature of goals during mission
execution. 
% All of these refinements are considered in order to further improve
% our approach in the context of temporal planning. They would allow
% further improvement in the overall execution of our agent despite the fact
% that the set of goals it will receive is not fully known until mission 
% completion.
In our domain such a requirement is critical as our AUV is in a
dynamic environment and it is far more cost-effective to use the
limited resources (scientists and on-station time) judiciously.  




% While there has already been work around the topic of robust
% \fcomment{This is just a quick presentation on potential future
%   direction} As we stated in the related works our approach does not
% really address dynamic controllability and has the more classic
% assumption present in many planning frameworks that time-points are
% controllable. A side effect of this is that in its current state it
% may result on the system to decide to defer action as late as
% possible. In our example, this would result on the AUV leaving Vent1
% as late as 19:00 making the rest of its plan brittle to any delay due
% for example to downward water currents on its way to the surface. This
% needs to be further addressed in the future and, especially, how our
% work can be integrated with work presented in \cite{morris01}.

% Further as of today, we consider that the qualification of the goal is
% predefined when the goal is submitted by the planner. It is possible
% though that part of this can be refined on some cases based on the
% nature of the goal. Looking back at our domain, one can note that the
% 2 goals provided are constrained differently on their start time;
% while {\em Sample Vent2} start time is limited only on its upper
% bound, the returning to surface conversely is constrained only on the
% lower bound of its start time. This difference hints on some of the
% issues we presented. While we do consider that explicit information of
% these goals help the plan execution to be improved when such
% information is not initially present. We also are aware that the
% nature of the constraints within the goal itself can help identify the
% best policy to be done. It is obvious that for Sampling Vent2 it is
% better to be proactive on the actions that contribute to this
% goal. Conversely, the other goal only matter if it appears fairly late
% in the plan which means that it is probably better to not start
% completing this part of the plan too aggressively. We plan to further
% explore how we can refine the distinction between the different
% policies by using the information provided by the constraints of the
% different objectives.
 
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai13"
%%% End: 
