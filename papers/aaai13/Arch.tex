\section{Planning Definitions}

To consider the overall planning and plan execution within an agent we
use the definition of a temporal domain, planning problem and solution
as provided by \cite{Nau:2004}:
\begin{definition}
  \label{def:domain}
  A temporal planning domain is a triple $D = ( \Lambda_\Phi, O, X )$, where:
  \begin{itemize}
  \item $\Lambda_\Phi$ is the set of all temporal databases that can be 
    defined with the constraints and the constant, variable, and relation 
    symbols in our representation.
  \item $O$ is a set of temporal planning operators.
  \item $X$ s a set of domain axioms.
  \end{itemize}
\end{definition}

\begin{definition}
  \label{def:problem}
  A temporal planning problem in $D$ is a tuple $P = ( D, \Phi_0, \Phi_g )$, where:
  \begin{itemize}
  \item $\Phi_0 = (F, C)$ is a database in $\Lambda_\Phi$ that satisfies the axioms of $X$.
    $\Phi_0$ represents an initial scenario that describes not only the initial 
    state of the domain but also the evolution predicted to take place 
    independently of the actions to be planned.
  \item $\Phi_g = (G, C_g)$ is a database that represents the goals of the problem 
    as a set $G$ of tokens together with a set $C_g$ of objects and temporal 
    constraints on variables of $G$. 
  \end{itemize}
\end{definition}

\begin{definition}
  \label{def:plan}
A plan is a set $\pi = \{a_1,...,a_k\}$ of actions, each
  being a partial instance of some operator in $O$.  

  $\pi$ is a solution of a problem $P = (D, \Phi_0, \Phi_g)$ iff
  there is a  database in $\lambda(\Phi_0, \pi)$\fcomment{Define
    lambda} that entails $\Phi_g$.
\end{definition}

\pcomment{Need to define what a:
  merged, slave, master, and active token is relative to a single token
  Also what an effect and condition is}
 
In this work, our focus is on executing a given plan $\pi$ which was
computed by the agent planner. However, in order to reflect the
dynamic interaction of the agent with its environment we need to
refine the definition of the sets $\Phi_0$ and $\Phi_g$. 

Indeed as the world evolve new observations (or refinement of
existing ones) are added into $\Phi_0$. Similarly, the agent operator
can request new future goals to be added to the agent $\Phi_g$ as
mission time advance. We note $\Phi_0(t)$ and $\Phi_g(t)$ the value 
of these sets at the time-point $t$. For the sake of simplicity we
 consider that the alteration of these sets is purely additive with
 time\fcomment{ In practice, their evolution is more complex as
   completed goals becomes observation or even a goal previously 
   requested can be cancelled by the operator. Still this assumption 
   largely simplify the problem description and our algorithm still
   works without it (even though it kind fo assume it)}:
\[ \forall \{t, t'\}: t \le t' \Rightarrow \Phi_0(t) \subseteq \Phi_0(t')
\wedge \Phi_g(t) \subseteq \Phi_g(t') \] 

%\fcomment{I will need to refine this and make it more convincing}
The dynamically growing nature of $\Phi_0$ reflects the cumul of
observation as the agent execute its plan $\pi$. In nominal 
situation new elements of $\Phi_0$ are refinements the plan 
-- for example by asserting that a planned command just
started\footnote{This preclude situations where new observations
  invalidate the plan which is out of the scope of this paper}. We
also consider that the agent can receive at any point new objectives
that will be added to $\Phi_g$. This assumption have an impact on how
it is preferable to handle plan execution. Indeed while deciding
when to start an action within the plan, one need to make  sure that
the execution of this action will not limit the ability fo the agent
to treat potential future emerging goals. In the light of it the agent
should at the best of its knowledge  try to balance the impact of the
next available action as early as possible or prefer to delay it in
the eventuality new goal occur. In our example, it was making sense to
go to the Grocery early, but on the other hand going back home to soon
would result on the current plan locking the agent -- within its current
plan -- at home until 8 pm. The solution providing the most freedom for
the agent was therefore for him to alternate between the two policies
depending on the action impact. 

In order to help the agent have a better knowledge on the nature of
the goals we do consider that each goal provide information on its
priority. In that putspose we define that $\Phi_g$ is partitioned into
2 sets:
\begin{itemize}
\item the internal goals $\Phi_{gi}$ which represent goals the agent
  {\em need} to maintain internally. These goals will be considered 
  as objectives that are not of the higher priority and therefore
  their actions can be deferred during execution.
\item the external goals $\Phi_{ge}$ which represents the goal
  received by the agent externally. As these goals are requested by
  the user, we consider them as to be of higher important -- ie the
  agent {\em wants} to execute them. Therefore, their execution should
  be preferrrably proactive.
\end{itemize}

At any point we need to evaluate an action within our plan $\pi$ we
consider that this plan is up to date and provide a solution of all 
the goals of both $\Phi_{gi}$ and $\Phi_{ge}$ that can reasonably  be
done within the current mission scope.


\section{Algorithm Draft}

As a new action can be dispatched for execution, the executive
needs to evaluate how it relates to the goals of the plan. Intuitively
if this action was generated by (or contribute to) an internal goal of
$\Phi_{gi}$ it needs to be taken proactively, while otherwise we can 
consider it as non-urgent. Therefore, when evaluating if the token
representing this action  within the plan the executive needs to do a
forward search on the causal links related to this token to see if
they lead to an internal goal as implemented in Algorithm \ref{DispatchToken}


\begin{algorithm}
\caption{The function $DispatchToken$ finds if there is a goal in $\Phi_{ge}$ that is connected
to the token, $t$, and, if so, dispatches the token.}
\label{DispatchToken}
\begin{algorithmic}
\Function{DispatchToken}{Token $T$}
\State $Boolean Goal$ = $SearchForGoal( T )$
\If{ $Goal$ $=$ True}
	\State \Return Dispatch $T$
\ElsIf{ $T$ start upper bound $ \leq $ upper bound for the current tick}
	\State \Return Dispatch $T$
\Else
	\State \Return Don't dispatch $T$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

This Algorithm is the central deciding point for how a
token should be dispatched. By finding out that a token is connected
to a goal in $\Phi_{ge}$, we conclude that the token is a sub-goal,
and thus dispatch it immediately being proactive.  On the other hand,
if the token is not connected to a goal then we defer dispatching it
until necessary.  This demonstrates our distinction between how we
dispatch tokens, proactive or deferred.

\begin{algorithm}
\caption{The function $SearchForGoal$ does a Forward search looking for a token that is
in the set $\Phi_{ge}$.}
\label{SearchForGoal}
\begin{algorithmic}
\Function{SearchForGoal}{ Token $T$ }
\ForAll{Actions(s), $A$, that $T$ is a Condition }
	\ForAll{Effect(s), $E$, of Action $A$ }
		\If{$E$ is a Goal in $\Phi_{ge}$}
			\State \Return True
		\ElsIf{$E$ is a Condition of an Action}
			\State \Return SearchForGoal( $E$ )
		\Else 
			\State \Return False
		\EndIf
	\EndFor
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

A crucial part for deciding how to dispatch a token is finding whether the token in question is 
connected to a goal in $\Phi_{ge}$, call it $G$. Algorithm \ref{SearchForGoal} does the function of 
searching for $G$ in a forward-search manner. We define a causal link within our plan that must be
meet in order for our search to function. 
\begin{definition}
\label{def:subgoalLink}
A causal link is defined as linking a goal as an effect of an action whose conditions are needed in order
to complete the goal and, thus, are subgoals. This link can be recursive as the conditions themselves 
may be the effect of an action causing a causal chain to build.
\end{definition}
During our search, if we find $G$ then we know that the original token
is part of the solution for completing $G$.  As such, we want to be
proactive with completing the token early so as to ultimately complete
$G$. If we don't find $G$ then the token has no connection to an
external request. The token still needs to be dispatched, however,
there is no one explicitly requesting it to be accomplished. Thus, no
reason to be proactive.

\subsection{Dynamic solution during planning}


Searching for a goal as Algorithm \ref{SearchForGoal} does can be
quite computational expensive particularly if there are many tokens
that are continuously being dispatched. Completing a full search every
time a token needs to be dispatch can severely slow down the execution
process, which needs to remain quick to ensure proper
execution. Therefore, our next algorithmic approach distributes the
full search within the creation of the plan.  Resulting in spreading out the full cost of the
search. In order to not repeatedly search the plan, we save the tokens
that are connected to a goal in $\Phi_{ge}$ found during the
search. In this way, we acquire a list of tokens, $List_{goals}$, that should be
dispatched early.

The algorithm uses the same dispatching method as algorithm \ref{DispatchToken}.
The big difference is that rather than searching for the goal using algorithm \ref{SearchForGoal}, 
it only searches the list, $List_{goals}$, to see if the token is in it. The actually searching for the goals and causally 
connected tokens happens in algorithms \ref{NotifyAdded}, \ref{NotifyRemoved}, \ref{NotifyActivated}.

\begin{algorithm}
\caption{Saves goals as they are added to plan}
\label{NotifyAdded}
\begin{algorithmic}
\Function{NotifyAdded}{ Token $T$ }
\If{$T$ is a Goal in $\Phi_{ge}$}
	\State Insert $T$ into $List_{goals}$
\EndIf 
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Removes the token after it is removed from the plan}
\label{NotifyRemoved}
\begin{algorithmic}
\Function{NotifyRemoved}{ Token $T$ }
	\State Remove $T$ from $List_{goals}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Searches for tokens connected to goals}
\label{NotifyActivated}
\begin{algorithmic}
\Function{NotifyActivated}{ Token $T$ }
\If{$T$ is a goal in $\Phi_{ge}$ or $T$ is linked to a goal through one causal link}
	\State Recursively search the reverse causal link and add the tokens into $List_{goals}$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Deactivates token in plan}
\label{NotifyDeactivated}
\begin{algorithmic}
\Function{NotifyDeactivated}{ Token $T$ }
\If{$T$ is not a goal and not one causally linked to a goal}
	\State Remove $T$ from $List_{goals}$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Searches plan when tokens are merged}
\label{NotifyMerged}
\begin{algorithmic}
\Function{NotifyMerged}{ Token $T$ }
\If{$T$ is goal in $List_{goals}$}
	\State Recursively search the reverse causal link of the active $T$ and add tokens to $List_{goals}$
\ElsIf{The active token of $T$ is in $List_{goals}$}
	\State Recursively search the reverse causal link of $T$ and add tokens to $List_{goals}$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Removes token when split}
\label{NotifySplit}
\begin{algorithmic}
\Function{NotifySplit}{ Token $T$ }
\If{$T$ is not a goal or not one causally linked to a goal}
	\State Remove $T$ from $List_{goals}$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

In order to distribute the search, we situate our algorithm within the
planning search which offers callback functions for when a token in
the plan is altered. Taking full advantage of the planning search, we
use a backwards search from the goal following the reverse causal link
to the connected tokens.  We fully search from the goals because we
know that all the tokens connected through the causal link are
sub-goal.  On the contrary, fully searching each token could be
wasteful because there is no certainty that it will be linked to a
goal and, therefore, could bring little value to our search. However,
some tokens may get added to the plan or linked to a goal after we
have already searched the goals. Therefore, for every token we do a
local forward search of one causal link to verify if it is connected
to a goal in our saved list.  If so, we do a full backwards search
from the token since it has now proven to be valuable. After the plan
has been searched, it is as easy as searching a list for a token to
see if it should be dispatched early or be deferred to later.




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai13"
%%% End: 
