\section{Planning Definitions}
\label{sec: defs}

The focus of this paper is on the possibility accommodating \emph{new
  objectives} arising during mission execution, with consequent need
to preserve context for their inclusion and subsequent execution and
that actions not deemed 'urgent' be deferred. Our robot generates and
dispatches a partial plan in continuum to a lower level controller
which further decomposes these plans towards atomic actions. These
actions ultimately actuate hardware on our AUV. The world responds by
sending sensory feedback as \emph{observations} up thru the
hierarchical controller \cite{mcgann08bdup,rajan12dup}. Details of the
controller are beyond the scope of this paper and are omitted without
loss of generality of the concepts described. We first consider some
basic representational definitions related to temporal planning
consistent with \cite{Nau:2004}:

\begin{definition}
  \label{def:domain}
  A temporal planning domain is a triple $D = ( \Lambda_\Phi, O, X )$, where:
  \begin{itemize}
  \item $\Lambda_\Phi$ is the set of all temporal databases that can
    be defined with constraint, constant, variable, and relation
    symbols in the representation.
  \item $O$ is a set of temporal planning operators.
  \item $X$ s a set of domain axioms.
  \end{itemize}
\end{definition}

A \emph{token} is a property that holds within a flexible temporal
interval which represents an action or a state associated with $O$ or
$X$ \cite{py10dup}. A temporal interval is delimited by two {\em
  timepoints} \cite{Boddy93} marking the possible values of its {\em
  start} and {\em end}.

\begin{definition}
  \label{def:problem}
  A temporal planning problem in $D$ is a tuple $P = ( D, \Phi_{obs}, \Phi_g )$, where:
  \begin{itemize}
  \item $\Phi_{obs} = (F, C)$ is a database in $\Lambda_\Phi$ that
    satisfies the axioms of $X$.  $\Phi_{obs}$ represents an initial
    scenario that describes initial state of the domain and
    observations.
  \item $\Phi_g = (G, C_g)$ is a database that represents the goals of
    the problem as a set $G$ of tokens together with a set $C_g$ of
    objects  and temporal constraints   on variables of $G$.
  \end{itemize}
\end{definition}

\begin{definition}
  \label{def:plan}
  A plan is a set $\pi = \{a_1,...,a_k\}$ of actions, each being a
  partial instance of some operator in $O$.  We define $\lambda$ as
  the state-transition function.  $\pi$ is a solution of a problem $P
  = (D, \Phi_{obs}, \Phi_g)$ iff there is a database in $\lambda(\Phi_{obs},
  \pi)$ that entails $\Phi_g$.
\end{definition}
 
While the focus is on executing plan $\pi$, to reflect the dynamic
interaction of the agent with the environment we need to refine the
definition of the sets $\Phi_{obs}$ and $\Phi_g$. As the world
evolves, new (or refinement of existing) observations are added to
$\Phi_{obs}$. Similarly, the agent operator can request new goals in
the future to be added to the agent's $\Phi_g$ as mission time
advances. For the sake of simplicity we consider that the alteration
of these sets is purely additive with time\footnote{In practice, their
  evolution is more complex as completed goals becomes observation;
  even a goal previously requested can be cancelled by the
  operator. While this assumption largely simplifies problem
  description, our algorithm works regardless.}:
\[ \forall \{t, t'\}: t \le t' \Rightarrow \Phi_{obs}(t) \subseteq \Phi_{obs}(t')
\wedge \Phi_g(t) \subseteq \Phi_g(t') \] 

where $\Phi_{obs}(t)$ and $\Phi_g(t)$ are the value of these sets at
time $t$.

$\Phi_{obs}$ grows dynamically, reflecting cumulative observations as
$\pi$ is executed. In nominal situations, new elements of $\Phi_{obs}$
are refinements of the plan -- for example by asserting that a planned
command just started\footnote{This precludes situations where new
  observations invalidate the plan; this is out of the scope of this
  paper}. The agent can also receive new objectives at any point with
those added to $\Phi_g$. This assumption has an impact on how it is
preferable to handle plan execution. Indeed while deciding when to
start an action within the plan, one need to make sure that the
execution of this action will not limit the ability of the agent to
treat potential future emerging goals. In the light of it the agent
should at the best of its knowledge try to balance the impact of the
next available action as early as possible or prefer to delay it in
the eventuality new goals occur. In our example, it was making sense
to go to Vent2 early, but on the other hand going back to the surface
too soon would result in locking the AUV -- within its current plan --
at the Surface until $8$ pm. The solution providing the most
flexibility for the AUV was therefore for it to alternate between the
two policies depending on the action impact.

In order to help the AUV have a better knowledge on the nature of the
goals we do consider that each goal provide information on its
priority. In that purpose, we define that $\Phi_g$ is partitioned into
two sets:

\begin{itemize}

\item the \emph{internal} goals $\Phi_{gi}$ which represent goals the
  agent {\em needs} to maintain internally. These goals will be
  considered as objectives that are not of the higher priority and
  therefore their actions can be deferred during execution.

\item the \emph{external} goals $\Phi_{ge}$ which represents the goal
  received by the agent externally. As these goals are requested by
  the user, we consider them as to be of higher important -- ie the
  agent {\em wants} to execute them. Therefore, their execution should
  be preferrrably proactive.

\end{itemize}

At any point we need to evaluate an action within our plan $\pi$ we
consider that this plan is up to date and provide a solution of all 
the goals of both $\Phi_{gi}$ and $\Phi_{ge}$ that can reasonably be
done within the current mission scope.

\section{Algorithmic Description}

As a new action can be dispatched for execution, the executive needs
to evaluate how it relates to the goals of the plan. Intuitively if
this action was generated by (or contributes to) an internal goal of
$\Phi_{gi}$ it needs to be taken proactively, while otherwise we can
consider it as non-urgent. Therefore, when evaluating if the token
representing this action within the plan the executive needs to do a
forward search on the causal links related to this token to see if
they lead to an internal goal as implemented in Algorithm
\ref{DispatchToken}.

\begin{definition}
\label{def:subgoalLink}
A goal $g$ is causally linked to a state value $c_i$ if there is an
action such as $g$ is an effect of this action and $c_i$ is one of its
conditions\footnote{For metric temporal plans, Allen Algebra
  \cite{allen84} is the basis for determining relationships between
  actions and is more general than pre-conditions in classical
  planning.}. 
% A causal link is defined as linking a goal as an effect of an action
% whose conditions are needed in order to complete the goal and, thus, are
% subgoals. This link can be recursive as the conditions themselves may
% be the effect of an action causing a causal chain to build.
\end{definition}

As a result all the conditions of this action can be assumed to be
subgoals with similar priority as $g$. 

\begin{algorithm} [H]
  \caption{\small The function $DispatchToken$ finds if there is a
    goal in $\Phi_{ge}$ that is connected to the token, $t$, and, if
    so, dispatches the token.}
  \label{alg:dispatch}
\label{DispatchToken}
\begin{algorithmic}
\small 
\Function{DispatchToken}{Token $T$}
\State Boolean $Goal = \textsc{SearchForGoal}( T )$
\If{ $Goal$ $=$ True}
	\State \Return Dispatch $T$
\ElsIf{ $T$ start upper bound $ \leq $ current tick}
	\State \Return Dispatch $T$
\Else
	\State \Return Don't dispatch $T$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:dispatch} is the central deciding  point for how a 
token should be dispatched.
It is called at every execution cycles and evaluates the next tokens to be
executed.  By finding out that a token is connected to a goal
 in $\Phi_{ge}$, we conclude that the token is a sub-goal,
and thus dispatch it immediately being proactive.  On the other hand,
if the token is not connected to a goal then we defer dispatching it
until necessary.  This demonstrates our distinction between how we
dispatch tokens, proactive or deferred.

\begin{algorithm} [H]
  \caption{\small The function $SearchForGoal$ does a Forward search
    looking for a token that is in the set $\Phi_{ge}$.}
\label{SearchForGoal}
\begin{algorithmic}
\small
\Function{SearchForGoal}{ Token $T$ }
\If{$T \in \Phi_{ge}$}
  \State \Return True
\Else 
  \State List $Fringe$ = \{ Actions that $T$ is Condition of \}
  \ForAll{Action(s), $A$, in $Fringe$ }
     \ForAll{Effects, $E$ of $A$}
       \If{$E \in \phi_{ge}$}
          \State \Return True
        \Else 
           \State List $L$ = \{ Action(s) that $E$ is Condition of \}
           \State $Fringe$ = $Fringe \cup L$ 
        \EndIf
      \EndFor
  \EndFor
\EndIf
\State \Return False
\EndFunction
\end{algorithmic}
\end{algorithm}

To inform this decision we use Algorithm \ref{SearchForGoal} which
search for a causally link between the token $T$ and token $G$ of
$\Phi_{ge}$ in a forward-search manner. During our search, 
if we find $G$ then we know that the original token is part of the 
solution for completing $G$.  As such, we want to be
proactive in ordeto utlimately complete $G$ as early as possible.
If we don't find $G$ then the token has no connection to an
external request. The token still needs to be eventually dispatched, 
however, there is no motivation to execute this action earlier than necessary.

This algorithm is equivalent to a breadth first search along the
planning structure starting from the action we need to
evaluate. Therefore, its complexity is $O(N+E)$ where $N$ is the
number of tokens within the plan and $E$ the number ofcausal links that relate
these tokens. 

%\subsection{Dynamic solution during planning}
%
%Searching for a goal as Algorithm \ref{SearchForGoal} does can be
%quite computational expensive particularly if there are many tokens
%that are continuously being dispatched. Completing a full search every
%time a token needs to be dispatched can severely slow down the
%execution process, which needs to remain quick to ensure proper
%execution. Therefore, our next algorithmic approach distributes the
%full search within the creation of the plan.  Resulting in spreading
%out the full cost of the search. In order to not repeatedly search the
%plan, we save the tokens that are connected to a goal in $\Phi_{ge}$
%found during the search. In this way, we acquire a list of tokens,
%$List_{goals}$, that should be dispatched early.
%
%An alternative solution is to embed the propagation of these value
%during the planning search. The algorithm uses the same dispatching
%method as algorithm \ref{DispatchToken}.  The difference is that
%rather than searching for the goal using algorithm
%\ref{SearchForGoal}, it only searches the list, $List_{goals}$, to see
%if the token is in it. The actually searching for the goals and
%causally connected tokens happens in algorithms \ref{NotifyActivated},
%\ref{NotifyMerged}.
%
%\begin{algorithm}[H]
%\caption{\small Saves goals as they are added to plan}
%\label{NotifyAdded}
%\begin{algorithmic}
%\Function{NotifyAdded}{ Token $T$ }
%\If{$T$ is a Goal in $\Phi_{ge}$}
%	\State Insert $T$ into $List_{goals}$
%\EndIf 
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm} [H]
%\caption{\small Removes the token after it is removed from the plan}
%\label{NotifyRemoved}
%\begin{algorithmic}
%\Function{NotifyRemoved}{ Token $T$ }
%	\State Remove $T$ from $List_{goals}$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Searches for tokens connected to goals}
%\label{NotifyActivated}
%\begin{algorithmic}
%\Function{NotifyActivated}{ Token $T$ }
%\If{$T$ is a goal in $\Phi_{ge}$ or $T$ is linked to a goal through one causal link}
%	\State Recursively search the reverse causal link and add the tokens into $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Deactivates token in plan}
%\label{NotifyDeactivated}
%\begin{algorithmic}
%\Function{NotifyDeactivated}{ Token $T$ }
%\If{$T$ is not a goal and not one causally linked to a goal}
%	\State Remove $T$ from $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Searches plan when tokens are merged}
%\label{NotifyMerged}
%\begin{algorithmic}
%\Function{NotifyMerged}{ Token $T$ }
%\If{$T$ is goal in $List_{goals}$}
%	\State Recursively search the reverse causal link of the active token merged with $T$ and add tokens to $List_{goals}$
%\ElsIf{The active token of $T$ is in $List_{goals}$}
%	\State Recursively search the reverse causal link of $T$ and add tokens to $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Removes token when split}
%\label{NotifySplit}
%\begin{algorithmic}
%\Function{NotifySplit}{ Token $T$ }
%\If{$T$ is not a goal or not one causally linked to a goal}
%	\State Remove $T$ from $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%In order to distribute the search, we situate our algorithm within the
%planning search of the Europa Planner,\cite{frank2003}, which offers 
%callback functions for when a token in the plan is altered. The majority
%of the searching happens when tokens are either activated or 
%merged. For a token to merge with another token it has to be compatible with
%another token already in the plan. Splitting happens when they are
%no longer compatible. We have designed our algorithm around the
%Europa Planner, however, we believe that the general approach
%can work on any other planner. 
%
%Taking full advantage of the planning search, we
%use a backwards search from the goal following the reverse causal link
%to the connected tokens.  We fully search from the goals because we
%know that all the tokens connected through the causal link are
%sub-goals. By contrast, fully searching each token could be
%wasteful because there is no certainty that it will be linked to a
%goal and, therefore, could bring little value to our search. However,
%some tokens may get added to the plan or linked to a goal after we
%have already searched the goals. Therefore, for every token we do a
%local forward search of one causal link to verify if it is connected
%to a goal in our saved list.  If so, we do a full backwards search
%from the token since it has now proven to be valuable. After the plan
%has been searched, it is as easy as searching a list for a token to
%see if it should be dispatched early or be deferred to later.
%
%This approach is potentially more costly than the previous algorithm 
%as it needs to do local updates whenever the plan is altered by the search
%including retracting past updates if a backtrack occur during the
%search. Still it is compelling in the fact that this cost occurs during
%planning reducing the decision problem during execution to simply
%check if the given action has been marked during planning. It has
%been the solution we have preferred within our system for this reason
%as it is functionally equivalent to previous algorithm while reducing
%extra computation cost as the plan is executed. Planning phases occur
%within the plan only when either the initial plan failed to execute or
%the set of goals has been altered. Therefore, it is safe to assume that
%planning should occur more sporadically than execution decisions 
%which then give an edge to this latter algorithmic solution. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai13"
%%% End: 
