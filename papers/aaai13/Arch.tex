\section{Executing a Flexible Temporal Plan}
\label{sec: defs}

The focus of this paper is on the possibility of accommodating new
objectives arising \emph{during} mission execution, while preserving
room for their inclusion, and subsequent execution with appropriate
policy decision applied to each task. Our robot generates and
dispatches a partial plan in parallel with a lower level controller,
which further decomposes these plans towards atomic actions; these
tasks ultimately actuate hardware on our AUV. The world responds by
sending sensory feedback as \emph{observations} up thru the
hierarchical controller \cite{mcgann08b,rajan12}. Details of the
controller are beyond the scope of this paper and are not directly
relevant to the concepts described.

In order to explain our execution method, we first consider some basic
definitions related to a plan consistent with \cite{Nau:2004}. A plan
has a given set of operators and states.  Operators link a set of
states, conditions, to a new set of states, effects. We refer to this
link as a causal link in the plan. Recursive causal links build a path
in the plan from an initial state into a goal state. The plan
structure is similar to the \texttt{STRIPS} model in the definition of
a plan.

Within the plan, each state has a {\em start}, $T_s$ and {\em end}, $T_e$ time. We
refer to these as {\em timepoints}. Each {\em timepoint} is actually a flexible
interval of time $[t, T]$, which includes all possible values between
$t$ and $T$.  A \emph{token} is a property that holds within two such
flexible intervals, or timepoints, which represents a state
\cite{py10}.  Such a representation is referred to as a flexible
temporal plan. Further, we assume that the plan dispatched to us is in
its dispatchable form as defined by \cite{mus98a}.  The role of the
executive is to identify and decide as time advance which {\em
  timepoint} withing the dispatched plan can be properly instantiated
without generating inconsistency within the plan. If no such solution
exist the executive can only report this to the planner and  wait for
a new plan solution. In the past, this decision was often supported by
a strict policy for every timepoints. However, as noted in
the example this can lead to issues in when to dispatch actions.

In order to handle such problems, we assume that some states have a
defined execution policy. This execution policy does impact how their
start time will be handled by the executive. For example, an
early execution policy  implies a preference to set the {\em token}'s
start with the smallest possible value without breaking the plan. We
consider that these  execution policies are determined beforehand by 
a user for a limited number of the {\em tokens}. Typically, the {\em
  tokens} that have a defined execution policy are the goal states 
of the plan. This leaves states without an execution policy, 
in which case we either derive a policy or use a default. In the rest
of this paper our default policy is set to a strict latest start,
however any policy can be used.


%\begin{definition}
%  \label{def:domain}
%  A temporal planning domain is a triple $D = ( \Lambda_\Phi, O, X )$, where:
%  \begin{itemize}
%  \item $\Lambda_\Phi$ is the set of all temporal databases that can
%    be defined with constraint, constant, variable, and relation
%    symbols in the representation.
%  \item $O$ is a set of temporal planning operators.
%  \item $X$ is a set of domain axioms.
%  \end{itemize}
%\end{definition}

%A temporal plan is a partially order graph of tasks where nodes
%are defined in metric time.
%Each task consists of when it starts and ends or its start
%and end timepoint in the plan. A \emph{token} is a task that holds over the 
%flexible temporal interval consisting of a start and end timepoint.

%A \emph{token} is a property that holds within a flexible temporal
%interval which represents an action or a state associated with $O$ or
%$X$ \cite{py10dup}. A temporal interval is delimited by two {\em
%  timepoints} \cite{Boddy93} marking the possible values of its {\em
%  start} and {\em end}.

%\begin{definition}
%  \label{def:problem}
%  A temporal planning problem in $D$ is a tuple $P = ( D, \Phi_{obs}, \Phi_g )$, where:
%  \begin{itemize}
%  \item $\Phi_{obs} = (F, C)$ is a database in $\Lambda_\Phi$ that
%    satisfies the axioms of $X$.  $\Phi_{obs}$ represents an initial
%    scenario that describes initial state of the domain and
%    observations.
%  \item $\Phi_g = (G, C_g)$ is a database that represents the goals of
%    the problem as a set $G$ of tokens together with a set $C_g$ of
%    objects  and temporal constraints   on variables of $G$.
%  \end{itemize}
%\end{definition}

%\begin{definition}
%  \label{def:plan}
%  A plan is a set $\pi = \{a_1,...,a_k\}$ of actions, each being a
%  partial instance of some operator in $O$.  We define $\lambda$ as
%  the state-transition function.  $\pi$ is a solution of a problem $P
%  = (D, \Phi_{obs}, \Phi_g)$ iff there is a database in $\lambda(\Phi_{obs},
%  \pi)$ that entails $\Phi_g$.
%\end{definition}
 
% While our focus is on executing a plan, to reflect the dynamic
% interaction of the agent with the environment, we need to refine the
% definition of the flexible plan given to us. As the world evolves, new
% (or a refinement of existing) observations are added to the
% plan. Similarly, the operator can request new goals to be added to the
% agent's plan as mission time advances. For the sake of simplicity, we
% consider that the alterations to the plan are purely additive with
% time\footnote{In practice, their evolution is more complex as
%   completed goals becomes observation; even a goal previously
%   requested can be cancelled by the operator. While this assumption
%   largely simplifies problem description, our algorithm works
%   regardless.}
%\[ \forall \{t, t'\}: t \le t' \Rightarrow \Phi_{obs}(t) \subseteq \Phi_{obs}(t')
%\wedge \Phi_g(t) \subseteq \Phi_g(t') \] 
%
%where $\Phi_{obs}(t)$ and $\Phi_g(t)$ are the value of these sets at
%time $t$.

% \kcomment{This para starts very similarly to text above.}The plan gets
% updated as observations are made from the plan being executed. In
% nominal situations, these new observations are refinements of the plan
% -- for example by asserting that a planned command has just
% started\footnote{We exclude situations where new observations
%   invalidate the plan rather than merely refining it.}. The agent can
% also receive new objectives at any time which are added to the
% plan. Therefore after refinements or changes are made, the plan is
% dispatched again to the executive with an up to date and valid
% solution.

We finallly assume also that the plan can evolve in the future due to
the introduction of new goals. This has an impact on how to
handle plan execution. While deciding when to initiate a state
transition within the plan, one needs to ensure that the execution
will not limit the ability of the agent to treat emergent goals. In
light of this, the agent should attempt to balance the impact of
execution on the next available action. We chose to execute as early
as possible or delay it with the eventuality that new goals might
occur. Going to \emph{Vent2} early is viable; however, going back to
the surface too early would result in blocking the AUV -- within its
current plan -- at the Surface until 20:00. The solution which
provides the most flexibility is for the agent to alternate between
the two policies.

%The nature of the goals are better characterized by inclusion of their
%priority. For that purpose, we define $\Phi_g$ to be partitioned in two:
%
%\begin{itemize}
%
%\item \emph{internal} goals $\Phi_{gi}$ which represent goals the
%  agent {\em needs} to maintain internally. These goals will be
%  considered as objectives that can be deferred during execution.
%
%\item \emph{external} goals $\Phi_{ge}$ which represents the goal
%  received by the agent from an external source, like a user. These are
%  considered to be of higher importance -- \ie the agent {\em wants}
%  to execute them. Therefore, their execution should preferably be
%  proactive.
%
%\end{itemize}
%
%At any time $t$ during action evaluating by the executive within
%$\pi$, the plan is an up to date and valid solution to the set of
%goals in $\Phi_g$.

% At any point we need to evaluate an action within plan $\pi$ we
% consider that this plan is up to date and provide a solution of all
% the goals of both $\Phi_{gi}$ and $\Phi_{ge}$ that can reasonably be
% done within the current mission scope.

\section{Algorithmic Description}
\label{sec:algo}


As a new state is to be executed \kcomment{what does it mean to
  execute a 'state'?}, the executive needs to evaluate how it relates
to the policies of other states in the plan. Intuitively, if a state
$S_1$ was generated by (or contributes to \kcomment{what does
  'contribute' mean?}) another state $S_2$, which has a defined
policy, then $S_1$ should be executed using the policy of
$S_2$. Therefore, while evaluating the token within the plan
representing $S_1$, the executive needs to do a forward search of the
causal links related to the token to see if it leads to another token
with a defined policy. Algorithm \ref{SearchForGoal} will discuss the
case when a token is connected to two or more different policies. Note
that our example only uses two policies for execution.  % Therefore,
% this issue will not occur in our example.

% as implemented in
% Algorithm \ref{DispatchToken}.

%\begin{definition}
%  \label{def:subgoalLink}
%  A goal $g$ is causally linked to a state value $c_i$ if
%  $g$ is an effect of an action $a$ and $c_i$ is a condition of $a$.
%  \footnote{For metric temporal plans, Allen Algebra
%    \cite{allen84} is the basis for determining relationships between
%    actions and is more general than pre-conditions in classical
%    planning.}.
%\end{definition}

%Consequently, all conditions of this action can be assumed to be
%subgoals with a similar policy as $g$.

\begin{algorithm} [H]
  \caption{\small The function $ExecutionPolicy$ uses the
    $SearchForPolicy$ to determine the execution policy for Token
    $T$. We use the two policies of earliest and latest start.}
  \label{alg:dispatch}
\label{ExecutionPolicy}
\begin{algorithmic}
\small 
\Function{ExecutionPolicy}{Token $T$}
\State Policy $P = \textsc{SearchForPolicy}( T )$
\If{ $P$ $=$ Policy(EarliestStart)}
	\State \Return Dispatch $T$
\ElsIf{ $T$ start upper bound $ \leq $ current tick}
	\State \Return Dispatch $T$
\Else
	\State \Return Don't dispatch $T$
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:dispatch} is key in determining how a token should
be executed, and it is called at every execution cycle while
evaluating the next set of tokens to be executed. It also plays the
role of the executive and while specific to our example, does not
impact the generality of Algorithm \ref{SearchForGoal}.

% Finding a token is connected
% to a goal in $\Phi_{ge}$, we conclude that the token is a sub-goal,
% and thus dispatch it proactively.  Conversely, if the token is not
% connected to a goal then we defer dispatching until necessary.  This
% demonstrates our distinction between how we dispatch tokens,
% proactive or deferred.

\begin{algorithm} [htb]
  \caption{\small The function $SearchForPolicy$ does a forward search
    along the causal links to determine a policy for Token $T$.}
  \label{SearchForGoal}
\begin{algorithmic}
  \small
  \Function{SearchForPolicy}{ Token $T$ }
  \If{$T$ has Policy}
  \State \Return $T \to Policy$
  \Else 
  \State Set $Policies$ = \{\}
  \State List $Fringe$ = \{ Actions that $T$ is Condition of \}
  \State Mark $T$
  \ForAll{Action(s), $A$, in $Fringe$ }
  \State Mark $A$
  \ForAll{Effects, $E$ of $A$}
  \State Mark $E$
  \If{$E$ has Policy(Any)}
  \State Insert $E \to Policy$ into $Policies$
  \EndIf
  \State List $L$ = \{ Action(s) that $E$ is Condition of \}
  \State $Fringe$ = $Fringe \cup \{L \cap \neg Marked \}$ 
  \EndFor
  \EndFor
  \EndIf
  \State \Return SelectPolicy($Policies$)
\EndFunction
\end{algorithmic}
\end{algorithm}


% Algorithm \ref{SearchForGoal} is used to search for a causal link
% between the token $T$ and token $G$ of $\Phi_{ge}$ using
% forward-search. If $G$ is found, then token $T$ is part of the
% solution for completing $G$.  As such, we want to be proactive in
% order to dispatch $G$ as early as possible.  If a $G$ is not found,
% then token $T$ has no connection to an external request and therefore
% there is no motivation to execute this action earlier than
% necessary. This algorithm is equivalent to a breadth first search
% along the planning structure starting from the action we need to
% evaluate. Its complexity is $O(N+E)$ \cite{corman} where $N$ is the
% number of tokens within the plan and $E$ the number of causal links
% that connect these tokens.

To inform Algorithm \ref{alg:dispatch}, we use Algorithm
\ref{SearchForGoal} which searches the causal links connected to token
$T$ using a forward-search.  If a token with a policy is found, then
it is added into the list of possible policies for $T$.  After
gathering all the policies connected to $T$, we use the $SelectPolicy$
function which determines which out of the set should be used. Herein
rises the issue of having multiply defined policies.  The function
$SelectPolicy$ could determine which policy to return in a number of
ways. Some examples are using a priority for each policy or having
additive operators for the policies \kcomment{This last sentence
  doesn't parse. What are 'some examples'?}.  In our case, the
earliest start policy will be returned because it is the only policy
we define.  The default policy of latest start will be used in
Algorithm \ref{alg:dispatch} if $T$ is not connected to a token with a
defined policy. \kcomment{I think this entire para needs to be
  carefully read and evaluated. Overuse of the same word (policy or
  examples) needs to be avoided. Plus this notion of examples is
  confusing. What examples are these?}
%As such, we want to be proactive in order to dispatch $G$ as early as possible. If a $G$ is
%not found, then token $T$ has no connection to an external goal and
%therefore there is no motivation to execute this action earlier than
%necessary. 
The functions \kcomment{which function?} complexity is $O(N+E)$ where
$N$ is the number of tokens and $E$ the number of causal links that
connect these tokens in the plan. 

An alternate version of Algorithm \ref{alg:dispatch} that propagated
policies through causal links during planning was also
attempted. While functionally equivalent, the overhead during
backtracking -- requiring removal of propagated information during
search -- along with continuous plan refinement process in our system,
did not demonstrate any significant performance benefit for
evaluation.

% any significant benefit in term of performance and could ben create
% important extra cost should the search for plan been misleader by the
% heuristic.


%\subsection{Dynamic solution during planning}
%
%Searching for a goal as Algorithm \ref{SearchForGoal} does can be
%quite computational expensive particularly if there are many tokens
%that are continuously being dispatched. Completing a full search every
%time a token needs to be dispatched can severely slow down the
%execution process, which needs to remain quick to ensure proper
%execution. Therefore, our next algorithmic approach distributes the
%full search within the creation of the plan.  Resulting in spreading
%out the full cost of the search. In order to not repeatedly search the
%plan, we save the tokens that are connected to a goal in $\Phi_{ge}$
%found during the search. In this way, we acquire a list of tokens,
%$List_{goals}$, that should be dispatched early.
%
%An alternative solution is to embed the propagation of these value
%during the planning search. The algorithm uses the same dispatching
%method as algorithm \ref{DispatchToken}.  The difference is that
%rather than searching for the goal using algorithm
%\ref{SearchForGoal}, it only searches the list, $List_{goals}$, to see
%if the token is in it. The actually searching for the goals and
%causally connected tokens happens in algorithms \ref{NotifyActivated},
%\ref{NotifyMerged}.
%
%\begin{algorithm}[H]
%\caption{\small Saves goals as they are added to plan}
%\label{NotifyAdded}
%\begin{algorithmic}
%\Function{NotifyAdded}{ Token $T$ }
%\If{$T$ is a Goal in $\Phi_{ge}$}
%	\State Insert $T$ into $List_{goals}$
%\EndIf 
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm} [H]
%\caption{\small Removes the token after it is removed from the plan}
%\label{NotifyRemoved}
%\begin{algorithmic}
%\Function{NotifyRemoved}{ Token $T$ }
%	\State Remove $T$ from $List_{goals}$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Searches for tokens connected to goals}
%\label{NotifyActivated}
%\begin{algorithmic}
%\Function{NotifyActivated}{ Token $T$ }
%\If{$T$ is a goal in $\Phi_{ge}$ or $T$ is linked to a goal through one causal link}
%	\State Recursively search the reverse causal link and add the tokens into $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Deactivates token in plan}
%\label{NotifyDeactivated}
%\begin{algorithmic}
%\Function{NotifyDeactivated}{ Token $T$ }
%\If{$T$ is not a goal and not one causally linked to a goal}
%	\State Remove $T$ from $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Searches plan when tokens are merged}
%\label{NotifyMerged}
%\begin{algorithmic}
%\Function{NotifyMerged}{ Token $T$ }
%\If{$T$ is goal in $List_{goals}$}
%	\State Recursively search the reverse causal link of the active token merged with $T$ and add tokens to $List_{goals}$
%\ElsIf{The active token of $T$ is in $List_{goals}$}
%	\State Recursively search the reverse causal link of $T$ and add tokens to $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}[H]
%\caption{\small Removes token when split}
%\label{NotifySplit}
%\begin{algorithmic}
%\Function{NotifySplit}{ Token $T$ }
%\If{$T$ is not a goal or not one causally linked to a goal}
%	\State Remove $T$ from $List_{goals}$
%\EndIf
%\EndFunction
%\end{algorithmic}
%\end{algorithm}
%
%In order to distribute the search, we situate our algorithm within the
%planning search of the Europa Planner,\cite{frank2003}, which offers 
%callback functions for when a token in the plan is altered. The majority
%of the searching happens when tokens are either activated or 
%merged. For a token to merge with another token it has to be compatible with
%another token already in the plan. Splitting happens when they are
%no longer compatible. We have designed our algorithm around the
%Europa Planner, however, we believe that the general approach
%can work on any other planner. 
%
%Taking full advantage of the planning search, we
%use a backwards search from the goal following the reverse causal link
%to the connected tokens.  We fully search from the goals because we
%know that all the tokens connected through the causal link are
%sub-goals. By contrast, fully searching each token could be
%wasteful because there is no certainty that it will be linked to a
%goal and, therefore, could bring little value to our search. However,
%some tokens may get added to the plan or linked to a goal after we
%have already searched the goals. Therefore, for every token we do a
%local forward search of one causal link to verify if it is connected
%to a goal in our saved list.  If so, we do a full backwards search
%from the token since it has now proven to be valuable. After the plan
%has been searched, it is as easy as searching a list for a token to
%see if it should be dispatched early or be deferred to later.
%
%This approach is potentially more costly than the previous algorithm 
%as it needs to do local updates whenever the plan is altered by the search
%including retracting past updates if a backtrack occur during the
%search. Still it is compelling in the fact that this cost occurs during
%planning reducing the decision problem during execution to simply
%check if the given action has been marked during planning. It has
%been the solution we have preferred within our system for this reason
%as it is functionally equivalent to previous algorithm while reducing
%extra computation cost as the plan is executed. Planning phases occur
%within the plan only when either the initial plan failed to execute or
%the set of goals has been altered. Therefore, it is safe to assume that
%planning should occur more sporadically than execution decisions 
%which then give an edge to this latter algorithmic solution. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai13"
%%% End: 
